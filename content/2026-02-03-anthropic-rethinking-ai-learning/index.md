+++
title = Anthropic is Rethinking AI Learning and So am I?"
date = 2025-09-11
+++

Anthropic recently put out a really interesting [paper](https://www.anthropic.com/research/AI-assistance-coding-skills) that tries to understand the impact of AI usage on learning, which anecdotally feels very in line with my personal experience so far using AI in both my work, and my own personal learning.

> "It’s unclear whether this cognitive offloading can prevent people from growing their skills on the job, or—in the case of coding—understanding the systems they’re building"

> "AI creates a potential tension: as coding grows more automated and speeds up work, humans will still need the skills to catch errors, guide output, and ultimately provide oversight for AI deployed in high-stakes environments. Does AI provide a shortcut to both skill development and increased efficiency? Or do productivity increases from AI assistance undermine skill development?"

What they tried to understand is very much with how I have felt about some of my use of AI so far. At the onset, its feels like I am hyper charging my code search and reasoning abilities, quickly being able to scan large industry-sized projects and immediately understand the overall system, the important areas of code that require more care and thought, and the potential areas of improvement as per the endless sources of code optimization that the model is trained on. Who hasn't experienced Claude or GPT telling them that their 5 line cosntructor can be optimized and re-organized better in 6 different ways?

On the other hand, I have also definitely felt the guiding hand of AI sometimes become the puppet master of the machine, especially when I'm either too busy or too lazy to deep dive into things myself. In truth, although AI has definitely made the immediate inertia of work higher, it does start to rear an ugly head midway into a project, where I have to keep reminding myself of some work that I didn't really do, but need to be tacitly familiar with to make progress on more complicated follow up tasks or debugging.

In truth, the hardest parts of mind-molding with my helpful AI side kicks is that, in the process of leveraging them to power up my innate abilities, I definitely am giving up the depth of my knowledge of each individual line of code, each small compiler error turned one-line fix, and each path of potential investigation when debugging a test failure or unexpected behavior. This isn't always a problem, especially when AI is smart enough to reach the end solution by itself, but its especially noticeable when, after round of thinking and spinning, it fails to actually address the problem at hand. At that point, the time it takes for me to essentially re-do all of the thinking it had done itself to catch myself up to speed and fix the issue does take a toll, and often ends up being a net negative in time as a result.

All this being said, I would definitely characterize myself as an AI gloomer not a doomer. My personal opinion is that AI agents shouldn't be treated any different than a compiler, albeit, much more complex and non-deterministic. Software developers would never blame the compiler when their provided code raises compilation errors, instead, using this as a feedback mechanism to fit their input to match the spec defined by the system. Using AI without any sort of internal feedback mechanism or back and forth akin to pair programming becomes a slow march to being a glorified prompt monkey, 

This aligns with a lot of what the Anthropic paper suggests, including the fact that AI delegation, progressive AI reliance, and iterative AI debugging, all uses of AI that progressively grow more and more to be complete cognitive offloading to AI, are the cases where AI based learning is the least successful. In turn, I think the lack of learning during the process is also the reason why these uses of AI tend to be the least valuable for me in terms of developer productivity too. Eventually, the buck stops with me, whether its signing off on the AI slop I've been given, or putting out a fire when it blows up 100 of our unit tests, and more often than not when I get to that point, if I've done too much congitive offloading, I am pretty cooked as far as being able to actually be productive and resolve whatever problems my benevolent sidekick has left me.

Anyways, I try to keep it short here but I'm also trying to keep this blog semi active, so I'm going to definitely be revisiting the places where I have let AI completely take the wheel, and where I'm stil trying to hold onto as much autonomy as I can continue to have in this weird cyborg reality we now live in.